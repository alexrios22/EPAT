{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7-final"},"colab":{"name":"MLT 2 - Predictive models.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"G6UIn7c6kZOY","colab_type":"text"},"source":["{[Click here to read this notebook in Google Colab](https://colab.research.google.com/drive/1z_VSTnTAueUPF4kNXWSqaPr4DAOxyQ7z)}\n","\n","<head><link rel = \"stylesheet\" href = \"https://drive.google.com/uc?id=1zYOH-_Mb9jOjRbQmghdhsmZ2g6xAwakk\"></head>\n","\n","<table class = \"header\"><tr>\n","    <th align = \"left\">EPAT Batch 45 | MLT 2, 2020\\05\\04</th>\n","    <th align = \"right\">Written by: Gaston Solari Loudet</th>\n","</tr></table>\n","\n","### Predictive Models\n","\n","> Welcome! You have recently joined as a quant analyst in a trading firm. Your supervisor has asked you to build and compare two machine learning models to predict the direction (up or down) of the next day's return of stock ZXV, given the traded volume, return and deliverable quantity for the current day. You, being a systematic and well organized analyst, approach to tackle this problem in a stepwise fashion, which is laid out in this notebook. Go through steps 1 to 7 and answer the questions along the way.\n","\n","#### Imports and downloading\n","\n","> [**[1.A]**] In this assignment we will be working with the Scikit Learn library for machine learning, along with Numpy and Pandas. In the next cell, fill in the blanks to import \"``sklearn``\" library:"]},{"cell_type":"code","metadata":{"id":"uNWE4o9Xi3Tv","colab_type":"code","colab":{}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","import pandas, numpy, sklearn.metrics, sklearn.linear_model, sklearn.preprocessing, sklearn.neighbors, sklearn.model_selection"],"execution_count":46,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_7tbBEC1i3T0","colab_type":"text"},"source":["> [**[1.B]**] Read the .CSV data file called \"``MLT_Assignment_data.csv``\" such that the \"``Date``\" column is the index, and save it in a DataFrame called \"``df``\". This is the daily data for a stock ZXV for 491 days. Check out the shape of the DataFrame."]},{"cell_type":"code","metadata":{"id":"F14mY3EVi3T1","colab_type":"code","colab":{},"tags":[]},"source":["############################################################################\n","# Download CSV file from GitHub repo for use outside from Jupyter.\n","URL = \"https://drive.google.com/uc?id=1fDesPqg-Er4NHoIbehHy809r-2nUlM6L\"\n","############################################################################\n","df = pandas.read_csv(URL, index_col = [0], parse_dates = True)\n","# Shape of \"df\"\n","print(f\"DataFrame for market data on stock ZXV: {df.shape[0]} rows; {df.shape[1]} columns\")\n","assert(df.shape[0] == 491) ## Verify that the answer above is correct.\n","print(f\"Column names:\\n{[x for x in df.columns]}\")"],"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":"DataFrame for market data on stock ZXV: 491 rows; 7 columns\nColumn names:\n['Previous Close', 'Close Price', 'return', 'Traded Volume', 'Deliverable Qty', 'Next_day_return', 'Next_day_direction']\n"}]},{"cell_type":"markdown","metadata":{"id":"wHdv9GHyi3T8","colab_type":"text"},"source":["> [**[1.C]**] In this step you get to know about the dataset you are working with better and answer some important questions. The DataFrame \"``df``\" consists of the following seven columns:\n","\n","> 1. \"``Previous Close``\": Previous day close price for the stock.\n","  2. \"``Close Price``\": Current day close price for the stock.\n","  3. \"``return``\": Current day return.\n","  4. \"``Traded Volume``\": Current day trading volume in number of shares.\n","  5. \"``Deliverable Qty``\": Quantity of shares which actually move from one investor's demat account to other on current day.\n","  6. \"``Next_day_return``\": Next day's return.\n","  7. \"``Next_day_direction``\": Direction of next day's return. \"1 = up\" and \"0 = down\".\n","\n","> You have to create a ML model that predicts the direction of the return on the next trading day. Between linear regression and logistic regression, which is a better algorithm for this task and why? What are some other ML algorithms that can be used for this task?\n","\n","Linear regression serves as a method for simulating the approximate linear relationship between a set of explanatory variables and an explained one, in which all of them are continuous (neither discrete nor boolean). The error term (\"epsilon\") implies the supposed distance between the modelled explained variable's value and the real one.\n","\n","Logistic regression simulates a relationship between a set of explanatory variable, and the probability that one explained variable of boolean nature that depends on the formers, is either one of the two possible values, or the other. Output is then continuous (a probability between 0 and 1) but what tries to conceive is an idea of \"proximity to 1\" so the error term then becomes an uncertainty metric.\n","\n","What we are trying to predict is a \"direction\": moreover, the sign of a one-dimensional vector. Positive or negative. The explained variable shall then be boolean, and a logistic regression shall suit our need for measuring the certainty of such being either positive or negative.\n","\n","Note that a boolean entity is a non-ordinal discrete entity (ej: categories) with the restriction of being able to adopt only two possible values. Hence, any classifying algorithm with just two possible categories should do well. For example: support vector machines, dense neural networks with softmax activation functions, Naive Bayes, etc.\n","\n","#### Preparing the data\n","\n","> [**[2.A]**] The ML model that predicts the direction of the return on the next trading day should use 3 features. Namely: \"``return``\", \"``Traded Volume``\" and \"``Deliverable Qty``\". Which column in DataFrame 'df' is the target variable?\n","\n","Column \"``Next_day_direction``\" may be the target variable. The learning algorithm should fill in binary outputs such as \"0/1\" or \"-1/1\".\n","\n","> [**[2.B]**] Before we use the data with the <a href = \"https://scikit-learn.org/\">Scikit Learn</a> library, we need to get it in the right form. We need to prepare 2 Numpy arrays, \"``X``\" for the features and \"``y``\" for the target variable. In the cell below, we create a feature matrix \"``X``\" containing all the 3 aforementioned features. In the following cell, replace the blanks with the target column name to create the target array."]},{"cell_type":"code","metadata":{"id":"R2IajAX5i3UD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"af93809a-6dc3-432a-a8c9-118d28aa113d","tags":[]},"source":["# feature matrix X\n","X = df[[\"return\", \"Traded Volume\", \"Deliverable Qty\"]].values\n","print(f\"Feature matrix 'X' ({X.ndim}-dimensional array): {X.shape[0]} rows, {X.shape[1]} columns.\")\n","# target variable y\n","y = df[\"Next_day_direction\"].values\n","print(f\" Target vector 'y' ({y.ndim}-dimensional array): {y.shape[0]} rows\")\n","assert(X.shape[0] == y.shape[0])"],"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":"Feature matrix 'X' (2-dimensional array): 491 rows, 3 columns.\n Target vector 'y' (1-dimensional array): 491 rows\n"}]},{"cell_type":"markdown","metadata":{"id":"iu6UGNZhi3UI","colab_type":"text"},"source":["#### Train-test split\n","\n","> [**[3]**] The next task is to split the avalilable data into train and test datasets. Once you train a model on the training dataset, you can check for its out of sample predictive accuracy on the test dataset. You are asked by your supervisor to keep 20% of your data as test data. (80:20 train-test split). Scikit learn provides a functionality to split the data using the \"``train_test_split``\" function. Put the correct value of the \"``test_size``\" parameter to conduct a 80 to 20 train-test split."]},{"cell_type":"code","metadata":{"id":"l1P1BV9Mi3UI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"b5ac05fb-90d5-481b-8eeb-5b17b1020b7e","tags":[]},"source":["X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n","                                    X, y, test_size = 1/5, random_state = 0)\n","# Shapes of \"X_train\", \"X_test\", \"y_train\" and \"y_test\".\n","print(f\"Training feature matrix: {X_train.shape[0]} rows, {X_train.shape[1]} columns.\")\n","print(f\" Training target matrix: {y_train.shape[0]} elements.\")\n","print(f\" Testing feature matrix: {X_test.shape[0]} rows, {X_test.shape[1]} columns.\")\n","print(f\"  Testing target matrix: {y_test.shape[0]} elements.\")"],"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":"Training feature matrix: 392 rows, 3 columns.\n Training target matrix: 392 elements.\n Testing feature matrix: 99 rows, 3 columns.\n  Testing target matrix: 99 elements.\n"}]},{"cell_type":"markdown","metadata":{"id":"uxofenlBi3UN","colab_type":"text"},"source":["#### Feature scaling\n","\n","> [**[4]**] As different features can have different scales and variance, we rescale all the features so that all of them have zero mean and unit standard deviation. This process is called \"feature standardization\" and can be performed using \"``StandardScaler``\"  from Scikit Learn. In the cell below, we standardize the \"``X_train``\" feature matrix. In the following cell, repeat the procedure for \"``X_test``\" so as to get a scaled version of \"``X_test``\"."]},{"cell_type":"code","metadata":{"id":"a4bfZwYmi3UN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"outputId":"a0f676ac-3ded-40ce-d196-0986e6aa6b87"},"source":["# Standardizing features in X_train\n","X_train_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X_train)\n","# Standardizing features in X_test\n","X_test_scaled = sklearn.preprocessing.StandardScaler().fit_transform(X_test)\n","# CHECK\n","c, i = [\"X_train\", \"_scaled\", \"X_test\", \"_scaled\"], [\"Mean\", \"StDv\"]\n","check = pandas.DataFrame(columns = c, index = i, \n","        data = [[X_train.mean(), X_train_scaled.mean(),\n","                 X_train.std(), X_train_scaled.std()],\n","                [X_test.mean(), X_test_scaled.mean(),\n","                 X_test.std(), X_test_scaled.std()]]).round(0)\n","check.head()"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":"        X_train  _scaled     X_test  _scaled\nMean  4512257.0     -0.0  5173718.0      1.0\nStDv  4486566.0      0.0  4812810.0      1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X_train</th>\n      <th>_scaled</th>\n      <th>X_test</th>\n      <th>_scaled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Mean</th>\n      <td>4512257.0</td>\n      <td>-0.0</td>\n      <td>5173718.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>StDv</th>\n      <td>4486566.0</td>\n      <td>0.0</td>\n      <td>4812810.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"zqbLudgDi3UT","colab_type":"text"},"source":["#### Fitting the models\n","\n","> [**[5]**] Based on past research on similar data, your supervisor has asked you to to fit and evaluate the following two models on the scaled training data:\n","\n","> 1. \"``Model_1``\": A Logistic Regression model, to which you are already familiar.\n","  2. \"``Model_2``\": k-Nearest-Neighbors, with \"$k = 5$\"... But what is this?\n","\n","The k-Nearest-Neighbors (\"<u>**kNN**</u>\") is also a classification algorithm like logistic regression, but uses a completely different methodology to classify the samples. It takes in a hyperparameter \"$k$\", decided by the user. Different values of \"$k$\" affect the decision boundary for the classification. In Scikit Learn, hyperparameter \"$k$\" is called \"``n_neighbors``\". From past experience, your supervisor tells you to use \"$k = 5$\".  \n","\n","In the next cell we write the code to create \"``Model_1``\". Execute the cell. On similar lines, we hae written the code for \"``Model_2``\" in the cell after that. Fill in the blanks in the following cell to fit the data to \"``Model_2``\" (i.e.: kNN with \"$k = 5$\") and execute both cells."]},{"cell_type":"code","metadata":{"id":"8IOyftlsi3UT","colab_type":"code","colab":{}},"source":["# Creating the model\n","Model_1 = sklearn.linear_model.LogisticRegression(solver = 'liblinear', random_state = None)\n","Model_2 = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 5)\n","# Training the model\n","Model_1.fit(X_train_scaled, y_train);\n","Model_2.fit(X_train_scaled, y_train);"],"execution_count":67,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yf_stjFci3UY","colab_type":"text"},"source":["#### k-fold cross validation on train data\n","\n","> [**[6]**] Now that both \"``Model_1``\" and \"``Model_2``\" are trained, we want to know the average accuracy of each model using k-fold cross validation on the training data itself. This will help us to gauge the likely out of sample performance of each model. In the cell below, we calculate the mean cross validated score (for a 10-fold cross validation) for \"``Model_1``\". Repeat the same for \"``Model_2``\" in the following cell. Based on these scores, which of the two models is likely to generalize well; for example, perform better on out of sample data?"]},{"cell_type":"code","metadata":{"id":"OUnJhclYi3UY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"c989d187-b3ea-4fa2-9d9c-3e19ddb74dad","tags":[]},"source":["cv = 10 ## Partitions.\n","# average cross-validated score for Model_1 and Model_2\n","ac1 = sklearn.model_selection.cross_val_score(Model_1, X_train_scaled, y_train, cv = cv, scoring = 'accuracy').mean().round(3)\n","ac2 = sklearn.model_selection.cross_val_score(Model_2, X_train_scaled, y_train, cv = cv, scoring = 'accuracy').mean().round(3)\n","print(\"Scores from 10-fold cross validation.\")\n","print(f\"---> Model_1: {numpy.round(1e4*ac1)/1e2}%  |  Model_2: {numpy.round(1e4*ac2)/1e2}%\")"],"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":"Scores from 10-fold cross validation.\n---> Model_1: 54.9%  |  Model_2: 53.6%\n"}]},{"cell_type":"markdown","metadata":{"id":"QHZC2s7Oi3Ue","colab_type":"text"},"source":["kNN model with \"``k = 5``\" has had a lower score than the logistic regression. This means that based in training data, the former will probably extrapolate better towards unseen (test) data in the future, in comparison to the latter (though not by much).\n","\n","#### Prediction and accuracy scores for test data\n","\n","> [**[7]**] In the cell below, we use \"``Model_1``\" to do out-of-sample prediction (i.e.: on test data) and print the accuracy scores. We are also printing the corresponding confusion matrix.  Repeat this procedure for \"``Model_2``\" in the following cell. Based on the accuracy scores for test data, which of the two models would you recommend?"]},{"cell_type":"code","metadata":{"id":"oeOwn0iQi3Ue","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"68bae009-b3fa-439a-fcce-b10245ecc4c9","tags":[]},"source":["# Model_1\n","# Prediction on test dataset\n","y_pred_test_1 = Model_1.predict(X_test_scaled)\n","score_1 = Model_1.score(X_test_scaled, y_test)\n","# Accuracy score \n","print(\"Accuracy for 'Model_1' on test data:\")\n","print(f\"---> %.2f%%\" % (100*score_1))\n","# Confusion matrix for prediction on test data\n","CM_1 = sklearn.metrics.confusion_matrix(y_test, y_pred_test_1)\n","print(f\"Confusion matrix for test prediction:\\n{CM_1}\")"],"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy for 'Model_1' on test data:\n---> 48.48%\nConfusion matrix for test prediction:\n[[27 16]\n [35 21]]\n"}]},{"cell_type":"code","metadata":{"id":"qU12cOIGi3Ug","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"0b915b6d-9aa7-49a3-b084-85dcc225e888","tags":[]},"source":["# Model_2\n","# Prediction on test dataset\n","y_pred_test_2 = Model_2.predict(X_test_scaled)\n","score_2 = Model_2.score(X_test_scaled, y_test)\n","# Accuracy score \n","print(\"Accuracy for 'Model_2' on test data:\")\n","print(f\"---> %.2f%%\" % (100*score_2))\n","# Confusion matrix for prediction on test data\n","CM_2 = sklearn.metrics.confusion_matrix(y_test, y_pred_test_2)\n","print(\"Confusion matrix for test prediction:\\n\", CM_2)"],"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy for 'Model_2' on test data:\n---> 37.37%\nConfusion matrix for test prediction:\n [[19 24]\n [38 18]]\n"}]},{"cell_type":"markdown","metadata":{},"source":["#### Conclusions\n","\n","The difference in precision is more than 10% favourable towards \"``Model_1``\". So Logistic Regression might likely perform quite better for this particular task. Also take into account that such algorithm is particularly optimized for binary classification, while \"kNN\" is a good extrapolator for more \"discrete\" sets. Both methods, however, had most of their fails in \"false positives\": expected bull days which ended up being bearish (lower rightmost numbers of matrices). It would be a better idea to create a deeper learning algorithm with a dense layer of \"tanh\" activation functions before the \"sigmoid\" logistic regressor."]}]}